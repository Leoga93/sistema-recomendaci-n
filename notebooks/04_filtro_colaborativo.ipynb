{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sistema de recomendaci칩n de filtro colaborativo basado en modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se busca realizar cinco recomendaciones de productos a usuarios basandose en la similitud con otros usuarios y los productos comprados. A trav칠s de un sistema de recomendaci칩n de filtro colaborativo basado en modelos implementando la estrategia de descomposici칩n de valor 칰nico (SVD) de la matriz usuario-item que muestra la interacci칩n de cada usuario con los productos de tecnolog칤a de una tienda global. Se utilizo inicialmente un dataset con un tama침o de 51290 filas y 24 columnas. La matriz usuario-item, se construyo filtrando el dataset para solo productos de tecnolog칤a, donde las filas son el id del usuario, las columnas el id del producto y los valores 칤ndica si el usuario compro el producto(1) o no compro el producto(0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carga exitosa:  (1301, 2375)\n"
     ]
    }
   ],
   "source": [
    "# Ruta del archivo\n",
    "base_dir = os.path.dirname(os.getcwd())\n",
    "url = os.path.join(base_dir, \"data\", \"processed\", \"users_matriz_items.csv\")\n",
    "\n",
    "# Cargando el archivo\n",
    "try: \n",
    "    data = pd.read_csv(url, sep=',', encoding='utf-8', index_col=0)\n",
    "    print(\"Carga exitosa: \", data.shape)\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"Error: Archivo no encontrado. Verifica la ruta.\")\n",
    "    \n",
    "except PermissionError:\n",
    "    print(\"Error: No tienes permiso para acceder al archivo.\")\n",
    "    \n",
    "except pd.errors.EmptyDataError:\n",
    "    print(\"Error: El archivo esta vac칤o\")\n",
    "    \n",
    "except pd.errors.ParserError:\n",
    "    print(\"Error: Fallo al analizar el archivo. Verifica el formato\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error inesperado: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separar datos de entrenamiento y prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tama침o conjunto de datos de entrenamiento: (1040, 2375)\n",
      "Tama침o conjunto de datos de prueba: (261, 2375)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Divisi칩n del conjunto de datos en entrenamiento y prueba\n",
    "train, test = train_test_split(\n",
    "    data, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "print(f\"Tama침o conjunto de datos de entrenamiento: {train.shape}\")\n",
    "print(f\"Tama침o conjunto de datos de prueba: {test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convertir a matriz dispersa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1040, 2375)\n",
      "(261, 2375)\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "train_matrix = csr_matrix(train)\n",
    "test_matrix = csr_matrix(test)\n",
    "\n",
    "print(train_matrix.shape)\n",
    "print(test_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aplicar TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "\n",
    "# Para tratar la indeterminaci칩n de signo primero se ajusta en los datos de entrenamiento y luego se usar ese mismo ajuste\n",
    "# Para transformar los datos de entrenamiento, prueba y datos nuevos.\n",
    "svd = TruncatedSVD(\n",
    "    n_components=443, \n",
    "    n_iter=7, \n",
    "    random_state=42, \n",
    "    tol=0.0, \n",
    "    n_oversamples=13, \n",
    "    power_iteration_normalizer='LU'\n",
    "    )\n",
    "\n",
    "# Ajustar el modelo a los datos de entrenamiento\n",
    "svd.fit(train_matrix)\n",
    "\n",
    "# Transformar los datos de entrenamiento\n",
    "U = svd.transform(train_matrix)\n",
    "s = np.diag(svd.singular_values_)\n",
    "Vt = svd.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1040, 443)\n",
      "(443, 443)\n",
      "(443, 2375)\n"
     ]
    }
   ],
   "source": [
    "# Por el momento se asume que los primeros valores singulares contendran la mayor informaci칩n de la matriz usuario-items\n",
    "# Verificando el la matriz de vectores singulares izquierdos\n",
    "print(U.shape)\n",
    "# Verificando el tama침o de la matriz de valores singulares\n",
    "print(s.shape)\n",
    "# Verificando el tama침o de la matriz de vectores singulares singulares derechos\n",
    "print(Vt.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reconstruir matriz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reconstruyendo la matriz latente apr칩ximada con la matriz de usuario y la matriz de items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "matriz_user = U\n",
    "matriz_items = Vt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruyendo la matriz apr칩xima a trav칠s del producto punto\n",
    "matriz_prediction = np.dot(matriz_user, matriz_items)\n",
    "\n",
    "# Convirtiendo a dataframe para un manejo m치s f치cil\n",
    "df_prediction = pd.DataFrame(\n",
    "    matriz_prediction, \n",
    "    index=train.index, \n",
    "    columns=train.columns\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruebas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Productos recomendados para el usuario BM-1140:\n",
      "Okidata Inkjet, Durable             0.021021\n",
      "Belkin Memory Card, Programmable    0.010778\n",
      "Okidata Calculator, White           0.009914\n",
      "StarTech Calculator, Wireless       0.009021\n",
      "Samsung Signal Booster, VoIP        0.008348\n",
      "Enermax Router, Bluetooth           0.007877\n",
      "Logitech Flash Drive, Bluetooth     0.007293\n",
      "HP Copy Machine, Color              0.007235\n",
      "Memorex Router, USB                 0.007159\n",
      "Motorola Smart Phone, Cordless      0.006951\n",
      "Name: BM-1140, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "url_base = os.path.dirname(os.getcwd())\n",
    "url = os.path.join(url_base, \"data\", \"processed\", \"processed_data.csv\")\n",
    "data = pd.read_csv(url, encoding=\"utf-8\", sep=\",\")\n",
    "\n",
    "def recomendar_productos(user_id, top=5):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        user_id (pandas.core.series.Series): Series with a length of 2375\n",
    "        top (int, optional): Number (n) of products to display. Defaults to 5.\n",
    "\n",
    "    Returns:\n",
    "        _type_: return the top n recommended products\n",
    "    \"\"\"\n",
    "    # Se obtiene los productos comprados por el usuario para no incluirlos en las predicciones\n",
    "    prod_comprados = train.loc[user_id]\n",
    "    prod_comprados = list(prod_comprados[prod_comprados > 0].index)\n",
    "    \n",
    "    # Obtener las puntuaciones predichas para el usuario\n",
    "    scores = df_prediction.loc[user_id]\n",
    "    scores = scores.drop(prod_comprados)\n",
    "    \n",
    "    # Ordenar descendente las puntuacioens\n",
    "    recommended_products = scores.sort_values(ascending=False).head(top)\n",
    "    \n",
    "    product_names = data.set_index(\"Product ID\")[\"Product Name\"].to_dict()\n",
    "    \n",
    "    recommended_products.index = recommended_products.index.map(lambda x: product_names.get(x, 'Desconocido'))\n",
    "        \n",
    "    return recommended_products\n",
    "\n",
    "usuario_id = \"BM-1140\"\n",
    "top_productos = recomendar_productos(usuario_id, top=10)\n",
    "\n",
    "print(f\"Productos recomendados para el usuario {usuario_id}:\")\n",
    "print(top_productos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicciones con los datos de prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matriz_test = svd.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recomendar productos multiplicando usuarios (test) con productos (items)\n",
    "predicciones = np.dot(matriz_test, matriz_items)\n",
    "df_predicciones = pd.DataFrame(predicciones, index=test.index, columns=test.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluaci칩n de rendimiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proporci칩n de variaci칩n explicada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se verifica cu치nta variaci칩n total explica el modelo con los componentes seleccionados. Si el valor es cercano a 1, significa que la mayoria de la  variaci칩n en los datos es explicada por los n_components seleccionados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"El porcentaje de varianza expl칤cada con {svd.n_components} componentes es {round(svd.explained_variance_ratio_.sum(), 2)} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precisi칩n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precisi칩n = Relevantes n recomendados @ k / cantidad de valores reales\n",
    "\n",
    "relevantes: interacciones reales\n",
    "\n",
    "n: intersecci칩n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_k(data_real, data_pred, k=6):\n",
    "    # Recorremos el n칰mero de indice de la longitud de los usuarios\n",
    "    precisiones = []\n",
    "    \n",
    "    for user in range(data_real.shape[0]):\n",
    "        \n",
    "        # Buscamos las interaciones reales que tiene el usuario con los productos\n",
    "        items_reales = set(np.asarray(data_real.iloc[user] > 0).nonzero()[0])\n",
    "        # Buscamos las interaciones predichas que tiene el usuario con los productos\n",
    "        items_pred = set(np.argsort(data_pred.iloc[user])[-k:][::-1])\n",
    "        \n",
    "        # Definimos para que la precisi칩n se calcule en los usuarios que realizaron interaciones\n",
    "        if len(items_reales) > 0:\n",
    "            interaciones = len(items_reales & items_pred)\n",
    "            precision = interaciones / min(k, len(items_reales))\n",
    "            precisiones.append(precision)\n",
    "    \n",
    "    return np.mean(precisiones) if precisiones else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_k(test, df_predicciones, k=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall@k\n",
    "\n",
    "recallk = Relevantes recomendados en el top k / total relevantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funci칩n para calcular el RecallK\n",
    "def recall_k(data_real, data_pred, k=5):\n",
    "    \n",
    "    # Comprobar si los datos son un dataframe o un array\n",
    "    if not isinstance(data_real, (pd.DataFrame, np.ndarray)) or not isinstance(data_pred, (pd.DataFrame, np.ndarray)):\n",
    "        raise TypeError(\"Los datos deben ser dataframe o ndarray\")\n",
    "    \n",
    "    if not isinstance(k, int) or k <= 0:\n",
    "        raise ValueError(\"El par치metro k debe ser un entero positivo.\")\n",
    "    \n",
    "    recalls = []\n",
    "\n",
    "    for user in range(len(data_real)):\n",
    "        items_real = set(np.asarray(data_real.iloc[user] > 0).nonzero()[0])\n",
    "        items_pred = set(np.argsort(data_pred.iloc[user])[-k:][::-1])\n",
    "        \n",
    "        if len(items_real) > 0:\n",
    "            interseccion = len(items_real & items_pred)\n",
    "            recall = interseccion / len(items_real)\n",
    "            recalls.append(recall)\n",
    "    \n",
    "    return np.mean(recalls) if recalls else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_k(test, df_predicciones, k=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAP(Mean Average Precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_average_precision(data_real, data_pred, k=5):\n",
    "    \n",
    "    # Verificando que los datos sean un dataframe o un array\n",
    "    if not isinstance(data_real, (pd.DataFrame, np.ndarray)) or not isinstance(data_pred, (pd.DataFrame, np.ndarray)):\n",
    "        raise TypeError(\"Los datos deben ser un dataframe o un array\")\n",
    "    \n",
    "    # Verificando que el valor k sea entero y mayor a 0\n",
    "    if not isinstance(k, int) or k <= 0:\n",
    "        raise ValueError(\"k debe ser un entero mayor a 0\")\n",
    "    \n",
    "    ap_list = []\n",
    "    \n",
    "    for user in range(len(data_real)):\n",
    "        items_reales = set(np.asarray(data_real.iloc[user] > 0).nonzero()[0])\n",
    "        items_pred = set(np.argsort(data_pred.iloc[user])[-k:][::-1])\n",
    "        \n",
    "        if len(items_reales) > 0:\n",
    "            hits = 0\n",
    "            sum_precisions = 0\n",
    "            for i, item in enumerate(items_pred):\n",
    "                if item in items_reales:\n",
    "                    hits += 1\n",
    "                    sum_precisions += hits / (i + 1)\n",
    "            \n",
    "            ap = sum_precisions / min(k, len(items_reales))\n",
    "            ap_list.append(ap)\n",
    "        \n",
    "    return np.mean(ap_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_average_precision(test, df_predicciones, k=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score(data_real, data_pred, k=5):\n",
    "    precision = precision_k(data_real, data_pred, k)\n",
    "    recall = recall_k(data_real, data_pred, k)\n",
    "    \n",
    "    if precision + recall == 0:\n",
    "        return 0\n",
    "    return 2 * (precision * recall) / (precision + recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(test, df_predicciones, k=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizaci칩n de hiperpar치metros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_k_scorer(estimator, X, y=None):\n",
    "    X_transformed = estimator.fit_transform(X)\n",
    "    score = precision_k(X, X_transformed, k=6)\n",
    "    print(\"Score obtenido:\", score)  # 游댌 Verifica si es NaN\n",
    "    return score if not np.isnan(score) else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "# Convertir la m칠trica en un scorer de scikit-learn\n",
    "scoring = make_scorer(precision_k_scorer, greater_is_better=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd.get_params(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'n_components': np.random.randint(100, 600, 30),  # N칰mero de componentes\n",
    "    'n_iter': [5, 7, 10],  # Iteraciones de optimizaci칩n\n",
    "    'random_state': [42],\n",
    "    'n_oversamples': np.random.randint(10, 100, 10),\n",
    "    'power_iteration_normalizer': ['auto', 'QR', 'LU', 'none']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "svd = TruncatedSVD()\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=svd,\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=5,  # N칰mero de combinaciones a probar\n",
    "    cv=3,\n",
    "    scoring=scoring,# Validaci칩n cruzada con 3 folds\n",
    "    random_state=42,\n",
    "    n_jobs=-1  # Usar todos los n칰cleos disponibles\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_search.fit(train_disperses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mejores par치metros:\", random_search.best_params_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_recomendacion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
