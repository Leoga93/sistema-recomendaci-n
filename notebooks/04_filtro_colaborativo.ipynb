{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sistema de recomendación de filtro colaborativo basado en modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se busca realizar cinco recomendaciones de productos a usuarios basandose en la similitud con otros usuarios y los productos comprados. A través de un sistema de recomendación de filtro colaborativo basado en modelos implementando la estrategia de descomposición de valor único (SVD) de la matriz usuario-item que muestra la interacción de cada usuario con los productos de tecnología de una tienda global. Se utilizo inicialmente un dataset con un tamaño de 51290 filas y 24 columnas. La matriz usuario-item, se construyo filtrando el dataset para solo productos de tecnología, donde las filas son el id del usuario, las columnas el id del producto y los valores índica si el usuario compro el producto(1) o no compro el producto(0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carga exitosa:  (1301, 2375)\n"
     ]
    }
   ],
   "source": [
    "# Ruta del archivo\n",
    "base_dir = os.path.dirname(os.getcwd())\n",
    "url = os.path.join(base_dir, \"data\", \"processed\", \"users_matriz_items.csv\")\n",
    "\n",
    "# Cargando el archivo\n",
    "try: \n",
    "    data = pd.read_csv(url, sep=',', encoding='utf-8', index_col=0)\n",
    "    print(\"Carga exitosa: \", data.shape)\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"Error: Archivo no encontrado. Verifica la ruta.\")\n",
    "    \n",
    "except PermissionError:\n",
    "    print(\"Error: No tienes permiso para acceder al archivo.\")\n",
    "    \n",
    "except pd.errors.EmptyDataError:\n",
    "    print(\"Error: El archivo esta vacío\")\n",
    "    \n",
    "except pd.errors.ParserError:\n",
    "    print(\"Error: Fallo al analizar el archivo. Verifica el formato\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error inesperado: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separar datos de entrenamiento y prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño conjunto de datos de entrenamiento: (1040, 2375)\n",
      "Tamaño conjunto de datos de prueba: (261, 2375)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# División del conjunto de datos en entrenamiento y prueba\n",
    "train, test = train_test_split(\n",
    "    data, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "print(f\"Tamaño conjunto de datos de entrenamiento: {train.shape}\")\n",
    "print(f\"Tamaño conjunto de datos de prueba: {test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convertir a matriz dispersa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1040, 2375)\n",
      "(261, 2375)\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "train_matrix = csr_matrix(train)\n",
    "test_matrix = csr_matrix(test)\n",
    "\n",
    "print(train_matrix.shape)\n",
    "print(test_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aplicar TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "\n",
    "# Para tratar la indeterminación de signo primero se ajusta en los datos de entrenamiento y luego se usar ese mismo ajuste\n",
    "# Para transformar los datos de entrenamiento, prueba y datos nuevos.\n",
    "svd = TruncatedSVD(\n",
    "    n_components=443, \n",
    "    n_iter=7, \n",
    "    random_state=42, \n",
    "    tol=0.0, \n",
    "    n_oversamples=13, \n",
    "    power_iteration_normalizer='LU'\n",
    "    )\n",
    "\n",
    "# Ajustar el modelo a los datos de entrenamiento\n",
    "svd.fit(train_matrix)\n",
    "\n",
    "# Transformar los datos de entrenamiento\n",
    "U = svd.transform(train_matrix)\n",
    "s = np.diag(svd.singular_values_)\n",
    "Vt = svd.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1040, 443)\n",
      "(443, 443)\n",
      "(443, 2375)\n"
     ]
    }
   ],
   "source": [
    "# Por el momento se asume que los primeros valores singulares contendran la mayor información de la matriz usuario-items\n",
    "# Verificando el la matriz de vectores singulares izquierdos\n",
    "print(U.shape)\n",
    "# Verificando el tamaño de la matriz de valores singulares\n",
    "print(s.shape)\n",
    "# Verificando el tamaño de la matriz de vectores singulares singulares derechos\n",
    "print(Vt.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reconstruir matriz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reconstruyendo la matriz latente apróximada con la matriz de usuario y la matriz de items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "matriz_user = U\n",
    "matriz_items = Vt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruyendo la matriz apróxima a través del producto punto\n",
    "matriz_prediction = np.dot(matriz_user, matriz_items)\n",
    "\n",
    "# Convirtiendo a dataframe para un manejo más fácil\n",
    "df_prediction = pd.DataFrame(\n",
    "    matriz_prediction, \n",
    "    index=train.index, \n",
    "    columns=train.columns\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruebas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Productos recomendados para el usuario BM-1140:\n",
      "Okidata Inkjet, Durable             0.021021\n",
      "Belkin Memory Card, Programmable    0.010778\n",
      "Okidata Calculator, White           0.009914\n",
      "StarTech Calculator, Wireless       0.009021\n",
      "Samsung Signal Booster, VoIP        0.008348\n",
      "Enermax Router, Bluetooth           0.007877\n",
      "Logitech Flash Drive, Bluetooth     0.007293\n",
      "HP Copy Machine, Color              0.007235\n",
      "Memorex Router, USB                 0.007159\n",
      "Motorola Smart Phone, Cordless      0.006951\n",
      "Name: BM-1140, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "url_base = os.path.dirname(os.getcwd())\n",
    "url = os.path.join(url_base, \"data\", \"processed\", \"processed_data.csv\")\n",
    "data = pd.read_csv(url, encoding=\"utf-8\", sep=\",\")\n",
    "\n",
    "def recomendar_productos(user_id, top=5):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        user_id (pandas.core.series.Series): Series with a length of 2375\n",
    "        top (int, optional): Number (n) of products to display. Defaults to 5.\n",
    "\n",
    "    Returns:\n",
    "        _type_: return the top n recommended products\n",
    "    \"\"\"\n",
    "    # Se obtiene los productos comprados por el usuario para no incluirlos en las predicciones\n",
    "    prod_comprados = train.loc[user_id]\n",
    "    prod_comprados = list(prod_comprados[prod_comprados > 0].index)\n",
    "    \n",
    "    # Obtener las puntuaciones predichas para el usuario\n",
    "    scores = df_prediction.loc[user_id]\n",
    "    scores = scores.drop(prod_comprados)\n",
    "    \n",
    "    # Ordenar descendente las puntuacioens\n",
    "    recommended_products = scores.sort_values(ascending=False).head(top)\n",
    "    \n",
    "    product_names = data.set_index(\"Product ID\")[\"Product Name\"].to_dict()\n",
    "    \n",
    "    recommended_products.index = recommended_products.index.map(lambda x: product_names.get(x, 'Desconocido'))\n",
    "        \n",
    "    return recommended_products\n",
    "\n",
    "usuario_id = \"BM-1140\"\n",
    "top_productos = recomendar_productos(usuario_id, top=10)\n",
    "\n",
    "print(f\"Productos recomendados para el usuario {usuario_id}:\")\n",
    "print(top_productos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicciones con los datos de prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matriz_test = svd.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recomendar productos multiplicando usuarios (test) con productos (items)\n",
    "predicciones = np.dot(matriz_test, matriz_items)\n",
    "df_predicciones = pd.DataFrame(predicciones, index=test.index, columns=test.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluación de rendimiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proporción de variación explicada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se verifica cuánta variación total explica el modelo con los componentes seleccionados. Si el valor es cercano a 1, significa que la mayoria de la  variación en los datos es explicada por los n_components seleccionados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"El porcentaje de varianza explícada con {svd.n_components} componentes es {round(svd.explained_variance_ratio_.sum(), 2)} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precisión"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precisión = Relevantes n recomendados @ k / cantidad de valores reales\n",
    "\n",
    "relevantes: interacciones reales\n",
    "\n",
    "n: intersección"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_k(data_real, data_pred, k=6):\n",
    "    # Recorremos el número de indice de la longitud de los usuarios\n",
    "    precisiones = []\n",
    "    \n",
    "    for user in range(data_real.shape[0]):\n",
    "        \n",
    "        # Buscamos las interaciones reales que tiene el usuario con los productos\n",
    "        items_reales = set(np.asarray(data_real.iloc[user] > 0).nonzero()[0])\n",
    "        # Buscamos las interaciones predichas que tiene el usuario con los productos\n",
    "        items_pred = set(np.argsort(data_pred.iloc[user])[-k:][::-1])\n",
    "        \n",
    "        # Definimos para que la precisión se calcule en los usuarios que realizaron interaciones\n",
    "        if len(items_reales) > 0:\n",
    "            interaciones = len(items_reales & items_pred)\n",
    "            precision = interaciones / min(k, len(items_reales))\n",
    "            precisiones.append(precision)\n",
    "    \n",
    "    return np.mean(precisiones) if precisiones else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_k(test, df_predicciones, k=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall@k\n",
    "\n",
    "recallk = Relevantes recomendados en el top k / total relevantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para calcular el RecallK\n",
    "def recall_k(data_real, data_pred, k=5):\n",
    "    \n",
    "    # Comprobar si los datos son un dataframe o un array\n",
    "    if not isinstance(data_real, (pd.DataFrame, np.ndarray)) or not isinstance(data_pred, (pd.DataFrame, np.ndarray)):\n",
    "        raise TypeError(\"Los datos deben ser dataframe o ndarray\")\n",
    "    \n",
    "    if not isinstance(k, int) or k <= 0:\n",
    "        raise ValueError(\"El parámetro k debe ser un entero positivo.\")\n",
    "    \n",
    "    recalls = []\n",
    "\n",
    "    for user in range(len(data_real)):\n",
    "        items_real = set(np.asarray(data_real.iloc[user] > 0).nonzero()[0])\n",
    "        items_pred = set(np.argsort(data_pred.iloc[user])[-k:][::-1])\n",
    "        \n",
    "        if len(items_real) > 0:\n",
    "            interseccion = len(items_real & items_pred)\n",
    "            recall = interseccion / len(items_real)\n",
    "            recalls.append(recall)\n",
    "    \n",
    "    return np.mean(recalls) if recalls else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_k(test, df_predicciones, k=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAP(Mean Average Precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_average_precision(data_real, data_pred, k=5):\n",
    "    \n",
    "    # Verificando que los datos sean un dataframe o un array\n",
    "    if not isinstance(data_real, (pd.DataFrame, np.ndarray)) or not isinstance(data_pred, (pd.DataFrame, np.ndarray)):\n",
    "        raise TypeError(\"Los datos deben ser un dataframe o un array\")\n",
    "    \n",
    "    # Verificando que el valor k sea entero y mayor a 0\n",
    "    if not isinstance(k, int) or k <= 0:\n",
    "        raise ValueError(\"k debe ser un entero mayor a 0\")\n",
    "    \n",
    "    ap_list = []\n",
    "    \n",
    "    for user in range(len(data_real)):\n",
    "        items_reales = set(np.asarray(data_real.iloc[user] > 0).nonzero()[0])\n",
    "        items_pred = set(np.argsort(data_pred.iloc[user])[-k:][::-1])\n",
    "        \n",
    "        if len(items_reales) > 0:\n",
    "            hits = 0\n",
    "            sum_precisions = 0\n",
    "            for i, item in enumerate(items_pred):\n",
    "                if item in items_reales:\n",
    "                    hits += 1\n",
    "                    sum_precisions += hits / (i + 1)\n",
    "            \n",
    "            ap = sum_precisions / min(k, len(items_reales))\n",
    "            ap_list.append(ap)\n",
    "        \n",
    "    return np.mean(ap_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_average_precision(test, df_predicciones, k=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score(data_real, data_pred, k=5):\n",
    "    precision = precision_k(data_real, data_pred, k)\n",
    "    recall = recall_k(data_real, data_pred, k)\n",
    "    \n",
    "    if precision + recall == 0:\n",
    "        return 0\n",
    "    return 2 * (precision * recall) / (precision + recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(test, df_predicciones, k=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimización de hiperparámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_k_scorer(estimator, X, y=None):\n",
    "    X_transformed = estimator.fit_transform(X)\n",
    "    score = precision_k(X, X_transformed, k=6)\n",
    "    print(\"Score obtenido:\", score)  # 🔍 Verifica si es NaN\n",
    "    return score if not np.isnan(score) else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "# Convertir la métrica en un scorer de scikit-learn\n",
    "scoring = make_scorer(precision_k_scorer, greater_is_better=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd.get_params(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'n_components': np.random.randint(100, 600, 30),  # Número de componentes\n",
    "    'n_iter': [5, 7, 10],  # Iteraciones de optimización\n",
    "    'random_state': [42],\n",
    "    'n_oversamples': np.random.randint(10, 100, 10),\n",
    "    'power_iteration_normalizer': ['auto', 'QR', 'LU', 'none']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "svd = TruncatedSVD()\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=svd,\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=5,  # Número de combinaciones a probar\n",
    "    cv=3,\n",
    "    scoring=scoring,# Validación cruzada con 3 folds\n",
    "    random_state=42,\n",
    "    n_jobs=-1  # Usar todos los núcleos disponibles\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_search.fit(train_disperses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mejores parámetros:\", random_search.best_params_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_recomendacion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
